{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Encrypted TOR Traffic with Boosting and Topological Data Analysis\n",
    "\n",
    "*HJ van Veen* - [MLWave](https://mlwave.com)\n",
    "\n",
    "**We establish strong baselines for both supervised and unsupervised detection of encrypted TOR traffic.**\n",
    "\n",
    "**Note: This article uses the 5-second lag dataset. For better comparison we will use the 15-second lag dataset in the near future.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Gradient Boosted Decision Trees (GBDT) is a very powerful learning algorithm for supervised learning on tabular data <a href=\"#References\">[1]</a>. Modern implementations include XGBoost <a href=\"#References\">[2]</a>, Catboost <a href=\"#References\">[3]</a>, LightGBM <a href=\"#References\">[4]</a> and scikit-learn's GradientBoostingClassifier <a href=\"#References\">[5]</a>. Of these, especially XGBoost has seen tremendous successes in machine learning competitions <a href=\"#References\">[6]</a>, starting with its introduction during the Higgs Boson Detection challenge in 2014 <a href=\"#References\">[7]</a>. The success of XGBoost can be explained on multiple dimensions: It is a robust implementation of the original algorithms, it is very fast -- allowing data scientists to quickly find better parameters <a href=\"#References\">[8]</a>, it does not suffer much from overfit, is scale-invariant, and it has an active community providing constant improvements, such as early stopping <a href=\"#References\">[9]</a> and GPU support <a href=\"#References\">[10]</a>.\n",
    "\n",
    "Anomaly detection algorithms automatically find samples that are different from regular samples. Many methods exist. We use the Isolation Forest in combination with nearest neighbor distances. The Isolation Forest works by randomly splitting up the data <a href=\"#References\">[11]</a>. Outliers, on average, are easier to isolate through splitting. Nearest neighbor distance looks at the summed distances for a sample and its five nearest neighbors. Outliers, on average, have a larger distance between their nearest neighbors than regular samples <a href=\"#References\">[12]</a>.\n",
    "\n",
    "Topological Data Analysis (TDA) is concerned with the meaning, shape, and connectedness of data <a href=\"#References\">[13]</a>. Benefits of TDA include: Unsupervised data exploration / automatic hypothesis generation, ability to deal with noise and missing values, invariance, and the generation of meaningful compressed summaries. TDA has shown efficient applications in a number of diverse fields: healthcare <a href=\"#References\">[14]</a>, computational biology <a href=\"#References\">[15]</a>, control theory <a href=\"#References\">[16]</a>, community detection <a href=\"#References\">[17]</a>, machine learning <a href=\"#References\">[18]</a>, sports analysis <a href=\"#References\">[19]</a>, and information security <a href=\"#References\">[20]</a>. One tool from TDA is the $MAPPER$ algorithm. $MAPPER$ turns data and data projections into a graph by covering it with overlapping intervals and clustering <a href=\"#References\">[21]</a>. To guide exploration, the nodes of the graph may be colored with a function of interest <a href=\"#References\">[22]</a>. There are an increasing number of implementations of $MAPPER$. We use the open source implementation KeplerMapper from scikit-TDA <a href=\"#References\">[23]</a>.\n",
    "\n",
    "The TOR network allows users to communicate and host content while preserving privacy and anonimity <a href=\"#References\">[24]</a>. As such, it can be used by dissidents and other people who prefer not to be tracked by commercial companies or governments. But these strong privacy and anonimity features are also attractive to criminals. A 2016 study in 'Survival - Global Politics and Strategy' found at least 57% of TOR websites are involved in illicit behavior, ranging from the trade in illegal arms, counterfeit ID documents, pornography, and drugs, money laundering & credit card fraud, and the sharing of violent material, such as bomb making tutorials and terrorist propaganda <a href=\"#References\">[25]</a>.\n",
    "\n",
    "Network Intrusion Detection Systems are a first line of defense for governments and companies <a href=\"#References\">[26]</a>. An undetected hacker will try to elevate their priviledges, moving from the weakest link to more hardened system-critical network nodes <a href=\"#References\">[27]</a>. If the hacker's goal is to get access to sensitive data (for instance: for resale -, industrial espionage -, or extortion purposes) then any stolen data needs to be exfiltrated. Similarly, cryptolockers often need to communicate with a command & control server outside the network. Depending on the level of sophistication of the malware or hackers, exfiltration may be open and visible, run encrypted through the TOR network in an effort to hide the destination, or use advanced DNS tunneling techniques.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "- Current Network Intrusion Detection Systems, much like the old spam detectors, rely mostly on rules, signatures, and anomaly detection. Labeled data is scarce. Writing rules is a very costly task requiring domain expertise. Signatures may fail to catch new types of attacks until they are updated. Anomalous/unusual behavior is not necessarily suspicous/adversarial behavior.\n",
    "- Machine Learning for Information Security suffers a lot from poor false positive rates. False positives lead to alarm fatigue and can swamp an intelligence analyst with irrelevant work.\n",
    "- Despite the possibility of false positives, it is often better to be safe than sorry. Suspicious network behavior, such as outgoing connections to the TOR network, require immediate attention. A network node can be shut down remotely, after which a security engineer can investigate the machine. The best practice of a multi-layered security makes this possible <a href=\"#References\">[28]</a>: Instead of a single firewall to rule them all, hackers can be detected in various stages of their network intrusion, up to the final step of data exfilitration.\n",
    "\n",
    "## Data\n",
    "\n",
    "We use a dataset written for the paper \"Characterization of Tor Traffic Using Time Based Features\" (Lashkari et al.) <a href=\"#References\">[29]</a>, graciously provided by the Canadian Institute for Cybersecurity <a href=\"#References\">[30]</a>. This dataset combines older research on nonTOR network traffic with more recently captured TOR traffic (both were created on the same network) <a href=\"#References\">[31]</a>. The data includes features that are more specific to the network used, such as the source and destination IP/Port, and a range of time-based features with a 5 second lag.\n",
    "\n",
    "|Feature|Type|Description|Time-based|\n",
    "|---|---|---|\n",
    "|'Source IP'|Object|Source IP4 Address. String with dots.|No|\n",
    "|' Source Port'|Float|Source Port sending packets.|No|\n",
    "|' Destination IP'|Object|Destination IP4 Address.|No|\n",
    "|' Destination Port'|Float|Destination Port receiving packets.|No|\n",
    "|' Protocol'|Float|Integer [5-17] denoting protocol used.|No|\n",
    "|' Flow Duration'|Float|Length of connection in seconds|Yes|\n",
    "|' Flow Bytes/s'|Float|Bytes per seconds send|Yes|\n",
    "|' Flow Packets/s'|Object|Packets per second send. Contains `\"infinity\"` strings.|Yes|\n",
    "|' Flow IAT Mean'|Float|Flow Inter Arrival Time.|Yes|\n",
    "|' Flow IAT Std'|Float||Yes|\n",
    "|' Flow IAT Max'|Float||Yes|\n",
    "|' Flow IAT Min'|Float||Yes|\n",
    "|'Fwd IAT Mean'|Float|Forward Inter Arrival Time.|Yes|\n",
    "|' Fwd IAT Std'|Float||Yes|\n",
    "|' Fwd IAT Max'|Float||Yes|\n",
    "|' Fwd IAT Min'|Float||Yes|\n",
    "|'Bwd IAT Mean'|Float|Backwards Inter Arrival Time.|Yes|\n",
    "|' Bwd IAT Std'|Float||Yes|\n",
    "|' Bwd IAT Max'|Float||Yes|\n",
    "|' Bwd IAT Min'|Float||Yes|\n",
    "|'Active Mean'|Float|Average amount of time in seconds before connection went idle.|Yes|\n",
    "|' Active Std'|Float||Yes|\n",
    "|' Active Max'|Float||Yes|\n",
    "|' Active Min'|Float||Yes|\n",
    "|'Idle Mean'|Float|Average amount of time in seconds before connection became active.|Yes|\n",
    "|' Idle Std'|Float|Zero variance feature.|Yes|\n",
    "|' Idle Max'|Float||Yes|\n",
    "|' Idle Min'|Float||Yes|\n",
    "|'label'|Object|Either `\"nonTOR\"` or `\"TOR\"`. ~17% TOR signal.|-|\n",
    "\n",
    "\n",
    "## Experimental setup\n",
    "\n",
    "- Supervised ML. We establish a strong baseline with XGBoost on the full data and on a subset (only time-based features, which generalize better to new domains). We follow the dataset standard of creating a 20% holdout validation set, and use 5-fold stratified cross-validation for parameter tuning <a href=\"#References\">[32]</a>. For tuning we use random search on sane parameter ranges, as random search is easy to implement and given enough time, will equal or beat more sophisticated methods <a href=\"#References\">[33]</a>. We do not use feature selection, but opt to let our learning algorithm deal with those. Missing values are also handled by XGBoost and not manually imputed or hardcoded.\n",
    "- Unsupervised ML. We use $MAPPER$ in combination with the Isolation Forest and the summed distances to the five nearest neighbors. We use an overlap percentage of 150% and 40 intervals per dimension for a total of 1600 hypercubes. Clustering is done with agglomerative clustering using the euclidean distance metric and 3 clusters per interval. For these experiments we use only the time-based features. We don't scale the data, despite only Isolation Forest being scale-invariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn import model_selection, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "There are string values `\"Infinity\"` inside the data, causing mixed types. We need to label-encode the target column. We turn the IP addresses into floats by removing the dots.\n",
    "\n",
    "We also create a subset of features by removing `Source Port`, `Source IP`, `Destination Port`, `Destination IP`, and `Protocol`. This to avoid overfitting/improve future generalization and focus only on the time-based features, like most other researchers have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CSV/Scenario-A/merged_5s.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.replace('Infinity', -1, inplace=True)\n",
    "df[\"label\"] = df[\"label\"].map({\"nonTOR\": 0, \"TOR\": 1})\n",
    "df[\"Source IP\"] = df[\"Source IP\"].apply(lambda x: float(x.replace(\".\", \"\")))\n",
    "df[\" Destination IP\"] = df[\" Destination IP\"].apply(lambda x: float(x.replace(\".\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Flow Duration',\n",
       " ' Flow Bytes/s',\n",
       " ' Flow Packets/s',\n",
       " ' Flow IAT Mean',\n",
       " ' Flow IAT Std',\n",
       " ' Flow IAT Max',\n",
       " ' Flow IAT Min',\n",
       " 'Fwd IAT Mean',\n",
       " ' Fwd IAT Std',\n",
       " ' Fwd IAT Max',\n",
       " ' Fwd IAT Min',\n",
       " 'Bwd IAT Mean',\n",
       " ' Bwd IAT Std',\n",
       " ' Bwd IAT Max',\n",
       " ' Bwd IAT Min',\n",
       " 'Active Mean',\n",
       " ' Active Std',\n",
       " ' Active Max',\n",
       " ' Active Min',\n",
       " 'Idle Mean',\n",
       " ' Idle Std',\n",
       " ' Idle Max',\n",
       " ' Idle Min']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_all = [c for c in df.columns if c not in \n",
    "            ['label']]\n",
    "\n",
    "features = [c for c in df.columns if c not in \n",
    "            ['Source IP',\n",
    "             ' Source Port',\n",
    "             ' Destination IP',\n",
    "             ' Destination Port',\n",
    "             ' Protocol',\n",
    "             'label']]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((84194, 23), 0.17231631707722642)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df[features])\n",
    "X_all = np.array(df[features_all])\n",
    "y = np.array(df.label)\n",
    "print(X.shape, np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local evaluation setup\n",
    "\n",
    "We create a stratified holdout set of 20%. Any modeling choices (such as parameter tuning) are guided by 5-fold stratified cross-validation on the remaining dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((67355, 23), (16839, 23))\n"
     ]
    }
   ],
   "source": [
    "splitter = model_selection.StratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=0.2,\n",
    "    random_state=0)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_holdout = X[train_index], X[test_index]\n",
    "    X_train_all, X_holdout_all = X_all[train_index], X_all[test_index]\n",
    "    y_train, y_holdout = y[train_index], y[test_index]\n",
    "    \n",
    "print(X_train.shape, X_holdout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold non-tuned XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "------------------------------------------------------------\n",
      "Fold: 0 ((53883, 23)/(13472, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       0.98      0.99      0.99     11150\n",
      "        TOR       0.93      0.93      0.93      2322\n",
      "\n",
      "avg / total       0.98      0.98      0.98     13472\n",
      "\n",
      "Confusion Matrix: \n",
      "[[10992   158]\n",
      " [  174  2148]]\n",
      "\n",
      "Log loss : 0.079730\n",
      "AUC      : 0.994554\n",
      "Accuracy : 0.975356\n",
      "Precision: 0.931483\n",
      "Recall   : 0.925065\n",
      "F1-score : 0.928263\n",
      "------------------------------------------------------------\n",
      "Fold: 1 ((53884, 23)/(13471, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       0.99      0.98      0.98     11150\n",
      "        TOR       0.92      0.93      0.92      2321\n",
      "\n",
      "avg / total       0.97      0.97      0.97     13471\n",
      "\n",
      "Confusion Matrix: \n",
      "[[10951   199]\n",
      " [  163  2158]]\n",
      "\n",
      "Log loss : 0.081962\n",
      "AUC      : 0.994286\n",
      "Accuracy : 0.973127\n",
      "Precision: 0.915571\n",
      "Recall   : 0.929772\n",
      "F1-score : 0.922617\n",
      "------------------------------------------------------------\n",
      "Fold: 2 ((53884, 23)/(13471, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       0.98      0.98      0.98     11150\n",
      "        TOR       0.93      0.92      0.92      2321\n",
      "\n",
      "avg / total       0.97      0.97      0.97     13471\n",
      "\n",
      "Confusion Matrix: \n",
      "[[10982   168]\n",
      " [  182  2139]]\n",
      "\n",
      "Log loss : 0.083443\n",
      "AUC      : 0.993946\n",
      "Accuracy : 0.974018\n",
      "Precision: 0.927178\n",
      "Recall   : 0.921586\n",
      "F1-score : 0.924373\n",
      "------------------------------------------------------------\n",
      "Fold: 3 ((53884, 23)/(13471, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       0.98      0.98      0.98     11150\n",
      "        TOR       0.92      0.93      0.93      2321\n",
      "\n",
      "avg / total       0.97      0.97      0.97     13471\n",
      "\n",
      "Confusion Matrix: \n",
      "[[10974   176]\n",
      " [  170  2151]]\n",
      "\n",
      "Log loss : 0.082286\n",
      "AUC      : 0.993697\n",
      "Accuracy : 0.974315\n",
      "Precision: 0.924366\n",
      "Recall   : 0.926756\n",
      "F1-score : 0.925559\n",
      "------------------------------------------------------------\n",
      "Fold: 4 ((53885, 23)/(13470, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       0.99      0.98      0.98     11149\n",
      "        TOR       0.93      0.93      0.93      2321\n",
      "\n",
      "avg / total       0.98      0.97      0.97     13470\n",
      "\n",
      "Confusion Matrix: \n",
      "[[10978   171]\n",
      " [  166  2155]]\n",
      "\n",
      "Log loss : 0.080182\n",
      "AUC      : 0.993894\n",
      "Accuracy : 0.974981\n",
      "Precision: 0.926483\n",
      "Recall   : 0.928479\n",
      "F1-score : 0.927480\n"
     ]
    }
   ],
   "source": [
    "model = xgboost.XGBClassifier(seed=0)\n",
    "print(model)\n",
    "\n",
    "skf = model_selection.StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=0)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    probas = model.predict_proba(X_test_fold)[:,1]\n",
    "    preds = (probas > 0.5).astype(int)\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    print(\"Fold: %d (%s/%s)\" %(i, X_train_fold.shape, X_test_fold.shape))\n",
    "    print(metrics.classification_report(y_test_fold, preds, target_names=[\"nonTOR\", \"TOR\"]))\n",
    "    print(\"Confusion Matrix: \\n%s\\n\"%metrics.confusion_matrix(y_test_fold, preds))\n",
    "    print(\"Log loss : %f\" % (metrics.log_loss(y_test_fold, probas)))\n",
    "    print(\"AUC      : %f\" % (metrics.roc_auc_score(y_test_fold, probas)))\n",
    "    print(\"Accuracy : %f\" % (metrics.accuracy_score(y_test_fold, preds)))\n",
    "    print(\"Precision: %f\" % (metrics.precision_score(y_test_fold, preds)))\n",
    "    print(\"Recall   : %f\" % (metrics.recall_score(y_test_fold, preds)))\n",
    "    print(\"F1-score : %f\" % (metrics.f1_score(y_test_fold, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning\n",
    "\n",
    "We found the below parameters by running a random gridsearch on the first fold in ~50 iterations (minimizing log loss). We use an AWS distributed closed-source auto-tuning library called \"Cher\" with the following parameter ranges:\n",
    "\n",
    "```\n",
    "\"XGBClassifier\": {\n",
    "    \"max_depth\": (2,12),\n",
    "    \"n_estimators\": (20, 2500),\n",
    "    \"objective\": [\"binary:logistic\"],\n",
    "    \"missing\": np.nan,\n",
    "    \"gamma\": [0, 0, 0, 0, 0, 0.01, 0.1, 0.2, 0.3, 0.5, 1., 10., 100.],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.1 ,0.1],\n",
    "    \"min_child_weight\": [1, 1, 1, 1, 2, 3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 15, 30, 60, 100, 1, 1, 1],\n",
    "    \"max_delta_step\": [0, 0, 0, 0, 0, 1, 2, 5, 8],\n",
    "    \"nthread\": -1,\n",
    "    \"subsample\": [i/100. for i in range(20,100)],\n",
    "    \"colsample_bytree\": [i/100. for i in range(20,100)],\n",
    "    \"colsample_bylevel\": [i/100. for i in range(20,100)],\n",
    "    \"reg_alpha\": [0, 0, 0, 0, 0, 0.00000001, 0.00000005, 0.0000005, 0.000005],\n",
    "    \"reg_lambda\": [1, 1, 1, 1, 2, 3, 4, 5, 1],\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"seed\": (0,999999)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier(base_score=0.5, colsample_bylevel=0.68, colsample_bytree=0.84,\n",
    "    gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=11,\n",
    "    min_child_weight=1, missing=None, n_estimators=1122, nthread=-1,\n",
    "    objective='binary:logistic', reg_alpha=0.0, reg_lambda=4,\n",
    "    scale_pos_weight=1, seed=189548, silent=True, subsample=0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold tuned XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=0.68, colsample_bytree=0.84,\n",
      "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=11,\n",
      "       min_child_weight=1, missing=None, n_estimators=1122, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0.0, reg_lambda=4,\n",
      "       scale_pos_weight=1, seed=189548, silent=True, subsample=0.98)\n",
      "------------------------------------------------------------\n",
      "Fold: 0 ((53883, 23)/(13472, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       1.00      0.99      0.99     11150\n",
      "        TOR       0.97      0.98      0.97      2322\n",
      "\n",
      "avg / total       0.99      0.99      0.99     13472\n",
      "\n",
      "Confusion Matrix: \n",
      "[[11081    69]\n",
      " [   49  2273]]\n",
      "\n",
      "Log loss : 0.023747\n",
      "AUC      : 0.999279\n",
      "Accuracy : 0.991241\n",
      "Precision: 0.970538\n",
      "Recall   : 0.978898\n",
      "F1-score : 0.974700\n",
      "------------------------------------------------------------\n",
      "Fold: 1 ((53884, 23)/(13471, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       1.00      0.99      0.99     11150\n",
      "        TOR       0.97      0.98      0.97      2321\n",
      "\n",
      "avg / total       0.99      0.99      0.99     13471\n",
      "\n",
      "Confusion Matrix: \n",
      "[[11071    79]\n",
      " [   52  2269]]\n",
      "\n",
      "Log loss : 0.029423\n",
      "AUC      : 0.999100\n",
      "Accuracy : 0.990275\n",
      "Precision: 0.966354\n",
      "Recall   : 0.977596\n",
      "F1-score : 0.971943\n",
      "------------------------------------------------------------\n",
      "Fold: 2 ((53884, 23)/(13471, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       0.99      0.99      0.99     11150\n",
      "        TOR       0.97      0.97      0.97      2321\n",
      "\n",
      "avg / total       0.99      0.99      0.99     13471\n",
      "\n",
      "Confusion Matrix: \n",
      "[[11081    69]\n",
      " [   68  2253]]\n",
      "\n",
      "Log loss : 0.030811\n",
      "AUC      : 0.998910\n",
      "Accuracy : 0.989830\n",
      "Precision: 0.970284\n",
      "Recall   : 0.970702\n",
      "F1-score : 0.970493\n",
      "------------------------------------------------------------\n",
      "Fold: 3 ((53884, 23)/(13471, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       0.99      1.00      0.99     11150\n",
      "        TOR       0.98      0.97      0.97      2321\n",
      "\n",
      "avg / total       0.99      0.99      0.99     13471\n",
      "\n",
      "Confusion Matrix: \n",
      "[[11098    52]\n",
      " [   68  2253]]\n",
      "\n",
      "Log loss : 0.023899\n",
      "AUC      : 0.999352\n",
      "Accuracy : 0.991092\n",
      "Precision: 0.977440\n",
      "Recall   : 0.970702\n",
      "F1-score : 0.974060\n",
      "------------------------------------------------------------\n",
      "Fold: 4 ((53885, 23)/(13470, 23))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       0.99      1.00      1.00     11149\n",
      "        TOR       0.98      0.97      0.98      2321\n",
      "\n",
      "avg / total       0.99      0.99      0.99     13470\n",
      "\n",
      "Confusion Matrix: \n",
      "[[11100    49]\n",
      " [   60  2261]]\n",
      "\n",
      "Log loss : 0.024206\n",
      "AUC      : 0.999168\n",
      "Accuracy : 0.991908\n",
      "Precision: 0.978788\n",
      "Recall   : 0.974149\n",
      "F1-score : 0.976463\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    probas = model.predict_proba(X_test_fold)[:,1]\n",
    "    preds = (probas > 0.5).astype(int)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"Fold: %d (%s/%s)\" %(i, X_train_fold.shape, X_test_fold.shape))\n",
    "    print(metrics.classification_report(y_test_fold, preds, target_names=[\"nonTOR\", \"TOR\"]))\n",
    "    print(\"Confusion Matrix: \\n%s\\n\"%metrics.confusion_matrix(y_test_fold, preds))\n",
    "    print(\"Log loss : %f\" % (metrics.log_loss(y_test_fold, probas)))\n",
    "    print(\"AUC      : %f\" % (metrics.roc_auc_score(y_test_fold, probas)))\n",
    "    print(\"Accuracy : %f\" % (metrics.accuracy_score(y_test_fold, preds)))\n",
    "    print(\"Precision: %f\" % (metrics.precision_score(y_test_fold, preds)))\n",
    "    print(\"Recall   : %f\" % (metrics.recall_score(y_test_fold, preds)))\n",
    "    print(\"F1-score : %f\" % (metrics.f1_score(y_test_fold, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       1.00      0.99      1.00     13937\n",
      "        TOR       0.97      0.98      0.98      2902\n",
      "\n",
      "avg / total       0.99      0.99      0.99     16839\n",
      "\n",
      "Confusion Matrix: \n",
      "[[13862    75]\n",
      " [   64  2838]]\n",
      "\n",
      "Log loss : 0.024852\n",
      "AUC      : 0.999289\n",
      "Accuracy : 0.991745\n",
      "Precision: 0.974253\n",
      "Recall   : 0.977946\n",
      "F1-score : 0.976096\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train) \n",
    "probas = model.predict_proba(X_holdout)[:,1]\n",
    "preds = (probas > 0.5).astype(int)\n",
    "\n",
    "print(metrics.classification_report(y_holdout, preds, target_names=[\"nonTOR\", \"TOR\"]))\n",
    "print(\"Confusion Matrix: \\n%s\\n\"%metrics.confusion_matrix(y_holdout, preds))\n",
    "print(\"Log loss : %f\" % (metrics.log_loss(y_holdout, probas)))\n",
    "print(\"AUC      : %f\" % (metrics.roc_auc_score(y_holdout, probas)))\n",
    "print(\"Accuracy : %f\" % (metrics.accuracy_score(y_holdout, preds)))\n",
    "print(\"Precision: %f\" % (metrics.precision_score(y_holdout, preds)))\n",
    "print(\"Recall   : %f\" % (metrics.recall_score(y_holdout, preds)))\n",
    "print(\"F1-score : %f\" % (metrics.f1_score(y_holdout, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "|Model|Precision|Recall|F1-Score\n",
    "|---|---|---|---|\n",
    "|Logistic Regression (Singh et al., 2018) <a href=\"#References\">[34]</a>|0.87|0.87|0.87|\n",
    "|SVM (Singh et al., 2018)|0.9|0.9|0.9|\n",
    "|Naïve Bayes (Singh et al., 2018)|0.91|0.6|0.7|\n",
    "|C4.5 Decision Tree + Feature Selection (Lashkari et al., 2017) <a href=\"#References\">[29]</a>|0.948|0.934|-|\n",
    "|Deep Learning (Singh et al., 2018)|0.95|0.95|0.95|\n",
    "|Random Forest (Singh et al., 2018)|0.96|0.96|0.96|\n",
    "|**XGBoost + Tuning**|**0.974**|**0.977**|**0.976**|\n",
    "\n",
    "### Holdout evaluation with all the available features\n",
    "\n",
    "Using all the features results in near perfect performance, suggesting \"leaky\" features (These features are not to be used for predictive modeling, but are there for completeness). Nevertheless we show how using all features also results in a strong baseline over previous research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     nonTOR       1.00      1.00      1.00     13937\n",
      "        TOR       1.00      1.00      1.00      2902\n",
      "\n",
      "avg / total       1.00      1.00      1.00     16839\n",
      "\n",
      "Confusion Matrix: \n",
      "[[13935     2]\n",
      " [    0  2902]]\n",
      "\n",
      "Log loss : 0.000455\n",
      "AUC      : 1.000000\n",
      "Accuracy : 0.999881\n",
      "Precision: 0.999311\n",
      "Recall   : 1.000000\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_all, y_train) \n",
    "probas = model.predict_proba(X_holdout_all)[:,1]\n",
    "preds = (probas > 0.5).astype(int)\n",
    "\n",
    "print(metrics.classification_report(y_holdout, preds, target_names=[\"nonTOR\", \"TOR\"]))\n",
    "print(\"Confusion Matrix: \\n%s\\n\"%metrics.confusion_matrix(y_holdout, preds))\n",
    "print(\"Log loss : %f\" % (metrics.log_loss(y_holdout, probas)))\n",
    "print(\"AUC      : %f\" % (metrics.roc_auc_score(y_holdout, probas)))\n",
    "print(\"Accuracy : %f\" % (metrics.accuracy_score(y_holdout, preds)))\n",
    "print(\"Precision: %f\" % (metrics.precision_score(y_holdout, preds)))\n",
    "print(\"Recall   : %f\" % (metrics.recall_score(y_holdout, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "|Model|Precision|Recall|Accuracy\n",
    "|---|---|---|---|\n",
    "|ANN (Hodo et al., 2017) <a href=\"References\">[35]</a>|0.983|0.937|0.991|\n",
    "|SVM (Hodo et al., 2017)|0.79|0.67|0.94|\n",
    "|ANN + Feature Selection (Hodo et al., 2017)|0.998|0.988|0.998|\n",
    "|SVM + Feature Selection (Hodo et al., 2017)|0.8|0.984|0.881|\n",
    "|**XGBoost + Tuning**|**0.999**|**1.**|**0.999**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topological Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kmapper as km\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble, cluster\n",
    "\n",
    "df = pd.read_csv(\"CSV/Scenario-A/merged_5s.csv\")\n",
    "df.replace('Infinity', -1, inplace=True)\n",
    "df[\" Flow Bytes/s\"] = df[\" Flow Bytes/s\"].apply(lambda x: float(x))\n",
    "df[\" Flow Packets/s\"] = df[\" Flow Packets/s\"].apply(lambda x: float(x))\n",
    "df[\"label\"] = df[\"label\"].map({\"nonTOR\": 0, \"TOR\": 1})\n",
    "df[\"Source IP\"] = df[\"Source IP\"].apply(lambda x: float(x.replace(\".\", \"\")))\n",
    "df[\" Destination IP\"] = df[\" Destination IP\"].apply(lambda x: float(x.replace(\".\", \"\")))\n",
    "df.fillna(-2, inplace=True)\n",
    "\n",
    "features = [c for c in df.columns if c not in \n",
    "            ['Source IP',\n",
    "             ' Source Port',\n",
    "             ' Destination IP',\n",
    "             ' Destination Port',\n",
    "             ' Protocol',\n",
    "             'label']]\n",
    "\n",
    "X = np.array(df[features])\n",
    "y = np.array(df.label)\n",
    "\n",
    "projector = ensemble.IsolationForest(random_state=0, n_jobs=-1)\n",
    "projector.fit(X)\n",
    "lens1 = projector.decision_function(X)\n",
    "\n",
    "mapper = km.KeplerMapper(verbose=3)\n",
    "lens2 = mapper.fit_transform(X, projection=\"knn_distance_5\")\n",
    "\n",
    "lens = np.c_[lens1, lens2]\n",
    "\n",
    "G = mapper.map(\n",
    "    lens,\n",
    "    X,\n",
    "    nr_cubes=40,\n",
    "    overlap_perc=1.5,\n",
    "    clusterer=cluster.AgglomerativeClustering(3))\n",
    "\n",
    "_ = mapper.visualize(\n",
    "    G,\n",
    "    custom_tooltips=y,\n",
    "    color_values=y,\n",
    "    path_html=\"tor-tda.html\",\n",
    "    inverse_X=X,\n",
    "    inverse_X_names=list(df[features].columns),\n",
    "    projected_X=lens,\n",
    "    projected_X_names=[\"IsolationForest\", \"KNN-distance 5\"],\n",
    "    title=\"Detecting encrypted Tor Traffic with Isolation Forest and Nearest Neighbor Distance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image of output\n",
    "![TDA image](https://i.imgur.com/1puM29w.jpg)\n",
    "\n",
    "### Link to output\n",
    "<a href=\"https://mlwave.github.io/tda/tor-tda.html\">TDA Tor Graph</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Both deep learning and unsupervised TDA may benefit from more data and rawer features. One strength of deep learning  is its ability to automaticly generate useful features. A properly tuned and architected RNN/LSTM or ConvNet on more data will likely beat or equal gradient boosting <a href=\"#References\">[36]</a>. Likewise for TDA: TDA is very good at extracting structure from raw time-series data. Using the preprocessed 5 second lag-features turns the problem more into a classification problem, than a temporal /forecasting problem.\n",
    "\n",
    "The XGBoost baseline can be further improved: Other authors showed feature selection to be effective at discarding noise. Stacked generalization can improve many pure classification problems, at the cost of an increased complexity and latency. Likewise with feature expansion through feature interactions, the score can be improved a small bit <a href=\"#References\">[37]</a>.\n",
    "\n",
    "The graph created with $MAPPER$ shows a concentration of anomalous samples that are predominantly nonTor traffic. This confirms our earlier note that anomalous behavior is not necessarily suspicious behavior. The separation could be better, but it is already possible to identify different types of Tor traffic, and see how they differ (an above average or below average `Flow Duration` can both signal Tor traffic.)\n",
    "\n",
    "The large `max_depth=11` found by XGBoost on this relatively small dataset signals that either the problem is very complex (and needs large complexity to be solved well), or that memorization of patterns is important for good performance on this dataset (larger `max_depth`'s find more feature interactions and are better at memorization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Thanks\n",
    "\n",
    "Thanks to dr. Satnam Singh and Balamurali A R for the inspiring [article](https://www.analyticsvidhya.com/blog/2018/07/using-power-deep-learning-cyber-security/). Thanks to my colleagues at Nubank InfoSec, especially <a href=\"https://github.com/jonasabreu\">Jonas Abreu</a>, for helpful discussions and consulting on domain expertise. Thanks to the Canadian Institute for Cybersecurity (dr. Lashkari et al.) for creating and providing the data used <a href=\"#References\">[38]</a>, and writing the original paper with great clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "**[1]** Freund, Schapire (1999). <br>[A short introduction to boosting](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf)\n",
    "<br>\n",
    "**[2]** Chen, Tianqi and Guestrin, Carlos (2016) <br>[XGBoost: A Scalable Tree Boosting System](http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)\n",
    "<br>\n",
    "**[3]** Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin (2017) <br>[CatBoost: unbiased boosting with categorical features](https://arxiv.org/abs/1706.09516)\n",
    "<br>\n",
    "**[4]** Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. (2017) <br>[LightGBM: A Highly Efficient Gradient Boosting Decision Tree.](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)\n",
    "<br>\n",
    "**[5]** Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E. (2011) <br>[Scikit-learn: Machine Learning in Python](http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf)\n",
    "<br>\n",
    "**[6]** Community (2014-). <br>[Awesome XGBoost](https://github.com/dmlc/xgboost/tree/master/demo)\n",
    "<br>\n",
    "**[7]** CERN and Kaggle (2014) <br>[Higgs Boson Machine Learning Challenge](https://www.kaggle.com/c/higgs-boson)\n",
    "<br>\n",
    "**[8]** Tianqi Chen on Quora (2015) <br>[What makes xgboost run much faster than many other implementations of gradient boosting?](https://www.quora.com/What-makes-xgboost-run-much-faster-than-many-other-implementations-of-gradient-boosting)\n",
    "<br>\n",
    "**[9]** Zygmunt Zając (2015) <br>[Early stopping](https://github.com/drivendata/countable-care-3rd-place/blob/master/src/xgboost_colsub.py)\n",
    "<br>\n",
    "**[10]** Rory Mitchell, Andrey Adinets, Thejaswi Rao, Eibe Frank (2018) <br>[XGBoost: Scalable GPU Accelerated Learning](https://arxiv.org/abs/1806.11248)\n",
    "<br>\n",
    "**[11]** Fei Tony Liu, Kai Ming Ting, Zhi-Hua Zhou (2008) <br>[Isolation Forest](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)\n",
    "<br>\n",
    "**[12]** Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. (2000) <br>[Efficient algorithms for mining outliers from large data sets](https://dl.acm.org/citation.cfm?id=335437)\n",
    "<br>\n",
    "**[13]** Gunnar Carlsson (2008) <br>[Topology and Data](https://web.stanford.edu/group/mmds/slides2008/carlsson.pdf)\n",
    "<br>\n",
    "**[14]** Devi Ramanan (2015) <br>[Identification of Type 2 Diabetes Subgroups through Topological Data Analysis of Patient Similarity](https://www.ayasdi.com/blog/healthcare/identification-of-type-2-diabetes-subgroups-through-topological-data-analysis-of-patient-similarity/)\n",
    "<br>\n",
    "**[15]** Pablo G. Cámara (2017) <br>[Topological methods for genomics: present and future directions](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5624534/)\n",
    "<br>\n",
    "**[16]** Wei Guo, Ashis Gopal Banerjee (2017) <br>[Identification of Key Features Using Topological Data Analysis for Accurate Prediction of Manufacturing System Outputs](https://www.researchgate.net/publication/314185934_Identification_of_Key_Features_Using_Topological_Data_Analysis_for_Accurate_Prediction_of_Manufacturing_System_Outputs)\n",
    "<br>\n",
    "**[17]** Mustafa Hajij, Bei Wang, Paul Rosen (2018) <br>[MOG: Mapper on Graphs for Relationship Preserving Clustering](https://arxiv.org/pdf/1804.11242.pdf)\n",
    "<br>\n",
    "**[18]** Anthony Bak (2015) <br>[Topology and Machine Learning](http://topology.cs.wisc.edu/Bak.pdf)\n",
    "<br>\n",
    "**[19]** Muthu Alagappan (2012) <br>[From 5 to 13: Redefining the Positions in Basketball](http://www.sloansportsconference.com/?p=5431)\n",
    "<br>\n",
    "**[20]** Marc Coudriau, Abdelkader Lahmadi, Jérôme François (2016) <br>[Topological analysis and visualisation of network monitoring data: Darknet case study](https://ieeexplore.ieee.org/document/7823920/)\n",
    "<br>\n",
    "**[21]** Gurjeet Singh, Facundo Mémoli, and Gunnar Carlsson (2007) <br>[Topological Methods for the Analysis of High Dimensional\n",
    "Data Sets and 3D Object Recognition](https://research.math.osu.edu/tgda/mapperPBG.pdf)\n",
    "<br>\n",
    "**[22]** P. Y. Lum, G. Singh, A. Lehman, T. Ishkanov, M. Vejdemo-Johansson, M. Alagappan, J. Carlsson & G. Carlsson (2009) <br>[Extracting insights from the shape of complex data using topology](https://www.nature.com/articles/srep01236)\n",
    "<br>\n",
    "**[23]** Hendrik Jacob van Veen, and Nathaniel Saul (2017) <br>[KeplerMapper](https://github.com/mlwave/kepler-mapper)\n",
    "<br>\n",
    "**[24]** Karsten Loesing and Steven J. Murdoch and Roger Dingledine (2010) <br>[A Case Study on Measuring Statistical Data in the Tor Anonymity Network](https://torproject.org/)\n",
    "<br>\n",
    "**[25]** Daniel Moore, Thomas Rid (2016) <br>[Cryptopolitik and the Darknet](https://www.tandfonline.com/doi/full/10.1080/00396338.2016.1142085)\n",
    "<br>\n",
    "**[26]** Stephen Northcutt, Judy Novak (2002) <br><a href=\"http://justpain.com/eBooks/Security/Network%20Intrusion%20Detection%20(New%20Riders).pdf\">Network Intrusion Detection, Third Edition</a>\n",
    "<br>\n",
    "**[27]** Justin Grana, David Wolpert, Joshua Neil, Dongping Xie, Tanmoy Bhattacharya, Russell Bent (2016) <br>[A Likelihood Ratio Detector for Identifying Within-Perimeter Computer Network Attacks.](https://arxiv.org/abs/1609.00104)\n",
    "<br>\n",
    "**[28]** Simon Denman (2012) <br>[Why multi-layered security is still the best defence](https://www.sciencedirect.com/science/article/pii/S1353485812700430)\n",
    "<br>\n",
    "**[29]** Arash Habibi Lashkari, Gerard Draper Gil, Mohammad Saiful Islam Mamun, Ali A. Ghorbani (2017) <br>[Characterization of Tor Traffic using Time based Features](https://www.researchgate.net/publication/314521450_Characterization_of_Tor_Traffic_using_Time_based_Features)\n",
    "<br>\n",
    "**[30]** Canadian Institute for Cybersecurity (Retrieved: 2018) <br>[Canadian Institute for Cybersecurity](https://www.unb.ca/cic/)\n",
    "<br>\n",
    "**[31]** Draper-Gil, G., Lashkari, A. H., Mamun, M. S. I., and Ghorbani, A. A. (2016). <br>[Characterization of encrypted and vpn traffic using time-related features](https://pdfs.semanticscholar.org/2886/2175a0426761c4f41fdbbec5b244026c4010.pdf)\n",
    "<br>\n",
    "**[32]** Trevor Hastie, Robert Tibshirani, Jerome H. Friedman (2001) <br>[The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "<br>\n",
    "**[33]** James Bergstra, Yoshua Bengio (2012) <br>[Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "<br>\n",
    "**[34]** Satnam Singh, Balamurali A R (2018) <br>[Using Deep Learning for Information Security](https://www.acalvio.com/wp-content/uploads/2018/07/Detecting-Anonymised-Network-Traffic-using-Deep-Learning.pdf)\n",
    "<br>\n",
    "**[35]** Elike Hodo, Xavier Bellekens, Ephraim Iorkyase, Andrew Hamilton, Christos Tachtatzis, Robert Atkinson (2017) <br>[Machine Learning Approach for Detection of nonTor Traffic](https://arxiv.org/abs/1708.08725)\n",
    "<br>\n",
    "**[36]** Gábor Melis, Chris Dyer, Phil Blunsom (2017) <br>[On the State of the Art of Evaluation in Neural Language Models](https://arxiv.org/abs/1707.05589)\n",
    "<br>\n",
    "**[37]** Marios Michailidis (2017) <br>[Investigating machine learning methods in recommender systems](http://discovery.ucl.ac.uk/10031000/1/Full_copy.pdf)\n",
    "<br>\n",
    "**[38]** Canadian Institute for Cybersecurity (2016) <br>[Tor-nonTor dataset (ISCXTor2016)](https://www.unb.ca/cic/datasets/tor.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
