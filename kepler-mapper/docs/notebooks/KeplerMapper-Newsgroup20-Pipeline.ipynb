{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeplerMapper & NLP examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newsgroups20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from kmapper import jupyter\n",
    "import kmapper as km\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We will use the Newsgroups20 dataset. This is a canonical NLP dataset containing 11314 labeled postings on 20 different newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SAMPLE', u\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\")\n",
      "('SHAPE', (11314,))\n",
      "('TARGET', 'rec.autos')\n"
     ]
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "X, y, target_names = np.array(newsgroups.data), np.array(newsgroups.target), np.array(newsgroups.target_names)\n",
    "print(\"SAMPLE\",X[0])\n",
    "print(\"SHAPE\",X.shape)\n",
    "print(\"TARGET\",target_names[y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection\n",
    "To project the unstructured text dataset down to 2 fixed dimensions, we will set up a function pipeline. Every consecutive function will take as input the output from the previous function.\n",
    "\n",
    "We will try out \"Latent Semantic Char-Gram Analysis followed by Isometric Mapping\".\n",
    "\n",
    "- TFIDF vectorize (1-6)-chargrams and discard the top 17% and bottom 5% chargrams. Dimensionality = 13967.\n",
    "- Run TruncatedSVD with 100 components on this representation. TFIDF followed by Singular Value Decomposition is called Latent Semantic Analysis. Dimensionality = 100.\n",
    "- Run Isomap embedding on the output from previous step to project down to 2 dimensions. Dimensionality = 2.\n",
    "- MinMaxScale the output from previous step. Dimensionality = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Composing projection pipeline length 3:\n",
      "Projections: TfidfVectorizer(analyzer='char', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.83, max_features=None, min_df=0.05,\n",
      "        ngram_range=(1, 6), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
      "       random_state=1729, tol=0.0)\n",
      "Isomap(eigen_solver='auto', max_iter=None, n_components=2, n_jobs=-1,\n",
      "    n_neighbors=5, neighbors_algorithm='auto', path_method='auto', tol=0)\n",
      "\n",
      "\n",
      "Distance matrices: False\n",
      "False\n",
      "False\n",
      "\n",
      "\n",
      "Scalers: None\n",
      "None\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "\n",
      "\n",
      "..Projecting on data shaped (11314,)\n",
      "\n",
      "..Projecting data using: \n",
      "\tTfidfVectorizer(analyzer='char', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.83, max_features=None, min_df=0.05,\n",
      "        ngram_range=(1, 6), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "\n",
      "\n",
      "..Created projection shaped (11314, 13967)\n",
      "..Projecting on data shaped (11314, 13967)\n",
      "\n",
      "..Projecting data using: \n",
      "\tTruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
      "       random_state=1729, tol=0.0)\n",
      "\n",
      "..Projecting on data shaped (11314, 100)\n",
      "\n",
      "..Projecting data using: \n",
      "\tIsomap(eigen_solver='auto', max_iter=None, n_components=2, n_jobs=-1,\n",
      "    n_neighbors=5, neighbors_algorithm='auto', path_method='auto', tol=0)\n",
      "\n",
      "\n",
      "..Scaling with: MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "\n",
      "('SHAPE', (11314, 2))\n"
     ]
    }
   ],
   "source": [
    "mapper = km.KeplerMapper(verbose=2)\n",
    "\n",
    "projected_X = mapper.fit_transform(X,\n",
    "    projection=[TfidfVectorizer(analyzer=\"char\",\n",
    "                                ngram_range=(1,6),\n",
    "                                max_df=0.83,\n",
    "                                min_df=0.05),\n",
    "                TruncatedSVD(n_components=100,\n",
    "                             random_state=1729),\n",
    "                Isomap(n_components=2,\n",
    "                       n_jobs=-1)],\n",
    "    scaler=[None, None, MinMaxScaler()])\n",
    "\n",
    "print(\"SHAPE\",projected_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "We cover the projection with 10 33%-overlapping intervals per dimension (10\\*10=100 cubes total).\n",
    "\n",
    "We cluster on the projection (but, note, we can also create an `inverse_X` to cluster on by vectorizing the original text data).\n",
    "\n",
    "For clustering we use Agglomerative Single Linkage Clustering with the \"cosine\"-distance and 3 clusters. Agglomerative Clustering is a good cluster algorithm for TDA, since it both creates pleasing informative networks, and it has strong theoretical garantuees (see [functor](https://en.wikipedia.org/wiki/Functor) and [functoriality](https://jeremykun.com/2013/07/14/functoriality/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping on data shaped (11314, 2) using lens shaped (11314, 2)\n",
      "\n",
      "Minimal points in hypercube before clustering: 3\n",
      "Creating 100 hypercubes.\n",
      "There are 0 points in cube_0 / 100\n",
      "Cube_0 is empty.\n",
      "\n",
      "There are 0 points in cube_1 / 100\n",
      "Cube_1 is empty.\n",
      "\n",
      "There are 18 points in cube_2 / 100\n",
      "Found 3 clusters in cube_2\n",
      "\n",
      "There are 42 points in cube_3 / 100\n",
      "Found 3 clusters in cube_3\n",
      "\n",
      "There are 27 points in cube_4 / 100\n",
      "Found 3 clusters in cube_4\n",
      "\n",
      "There are 5 points in cube_5 / 100\n",
      "Found 3 clusters in cube_5\n",
      "\n",
      "There are 3 points in cube_6 / 100\n",
      "Found 3 clusters in cube_6\n",
      "\n",
      "There are 0 points in cube_7 / 100\n",
      "Cube_7 is empty.\n",
      "\n",
      "There are 0 points in cube_8 / 100\n",
      "Cube_8 is empty.\n",
      "\n",
      "There are 0 points in cube_9 / 100\n",
      "Cube_9 is empty.\n",
      "\n",
      "There are 7 points in cube_10 / 100\n",
      "Found 3 clusters in cube_10\n",
      "\n",
      "There are 351 points in cube_11 / 100\n",
      "Found 3 clusters in cube_11\n",
      "\n",
      "There are 818 points in cube_12 / 100\n",
      "Found 3 clusters in cube_12\n",
      "\n",
      "There are 28 points in cube_13 / 100\n",
      "Found 3 clusters in cube_13\n",
      "\n",
      "There are 41 points in cube_14 / 100\n",
      "Found 3 clusters in cube_14\n",
      "\n",
      "There are 7 points in cube_15 / 100\n",
      "Found 3 clusters in cube_15\n",
      "\n",
      "There are 5 points in cube_16 / 100\n",
      "Found 3 clusters in cube_16\n",
      "\n",
      "There are 1 points in cube_17 / 100\n",
      "Cube_17 is empty.\n",
      "\n",
      "There are 2 points in cube_18 / 100\n",
      "Cube_18 is empty.\n",
      "\n",
      "There are 0 points in cube_19 / 100\n",
      "Cube_19 is empty.\n",
      "\n",
      "There are 30 points in cube_20 / 100\n",
      "Found 3 clusters in cube_20\n",
      "\n",
      "There are 374 points in cube_21 / 100\n",
      "Found 3 clusters in cube_21\n",
      "\n",
      "There are 201 points in cube_22 / 100\n",
      "Found 3 clusters in cube_22\n",
      "\n",
      "There are 19 points in cube_23 / 100\n",
      "Found 3 clusters in cube_23\n",
      "\n",
      "There are 100 points in cube_24 / 100\n",
      "Found 3 clusters in cube_24\n",
      "\n",
      "There are 101 points in cube_25 / 100\n",
      "Found 3 clusters in cube_25\n",
      "\n",
      "There are 30 points in cube_26 / 100\n",
      "Found 3 clusters in cube_26\n",
      "\n",
      "There are 136 points in cube_27 / 100\n",
      "Found 3 clusters in cube_27\n",
      "\n",
      "There are 11 points in cube_28 / 100\n",
      "Found 3 clusters in cube_28\n",
      "\n",
      "There are 6 points in cube_29 / 100\n",
      "Found 3 clusters in cube_29\n",
      "\n",
      "There are 42 points in cube_30 / 100\n",
      "Found 3 clusters in cube_30\n",
      "\n",
      "There are 126 points in cube_31 / 100\n",
      "Found 3 clusters in cube_31\n",
      "\n",
      "There are 19 points in cube_32 / 100\n",
      "Found 3 clusters in cube_32\n",
      "\n",
      "There are 9 points in cube_33 / 100\n",
      "Found 3 clusters in cube_33\n",
      "\n",
      "There are 183 points in cube_34 / 100\n",
      "Found 3 clusters in cube_34\n",
      "\n",
      "There are 144 points in cube_35 / 100\n",
      "Found 3 clusters in cube_35\n",
      "\n",
      "There are 34 points in cube_36 / 100\n",
      "Found 3 clusters in cube_36\n",
      "\n",
      "There are 179 points in cube_37 / 100\n",
      "Found 3 clusters in cube_37\n",
      "\n",
      "There are 161 points in cube_38 / 100\n",
      "Found 3 clusters in cube_38\n",
      "\n",
      "There are 7 points in cube_39 / 100\n",
      "Found 3 clusters in cube_39\n",
      "\n",
      "There are 31 points in cube_40 / 100\n",
      "Found 3 clusters in cube_40\n",
      "\n",
      "There are 68 points in cube_41 / 100\n",
      "Found 3 clusters in cube_41\n",
      "\n",
      "There are 31 points in cube_42 / 100\n",
      "Found 3 clusters in cube_42\n",
      "\n",
      "There are 17 points in cube_43 / 100\n",
      "Found 3 clusters in cube_43\n",
      "\n",
      "There are 54 points in cube_44 / 100\n",
      "Found 3 clusters in cube_44\n",
      "\n",
      "There are 18 points in cube_45 / 100\n",
      "Found 3 clusters in cube_45\n",
      "\n",
      "There are 52 points in cube_46 / 100\n",
      "Found 3 clusters in cube_46\n",
      "\n",
      "There are 202 points in cube_47 / 100\n",
      "Found 3 clusters in cube_47\n",
      "\n",
      "There are 175 points in cube_48 / 100\n",
      "Found 3 clusters in cube_48\n",
      "\n",
      "There are 0 points in cube_49 / 100\n",
      "Cube_49 is empty.\n",
      "\n",
      "There are 36 points in cube_50 / 100\n",
      "Found 3 clusters in cube_50\n",
      "\n",
      "There are 60 points in cube_51 / 100\n",
      "Found 3 clusters in cube_51\n",
      "\n",
      "There are 84 points in cube_52 / 100\n",
      "Found 3 clusters in cube_52\n",
      "\n",
      "There are 74 points in cube_53 / 100\n",
      "Found 3 clusters in cube_53\n",
      "\n",
      "There are 59 points in cube_54 / 100\n",
      "Found 3 clusters in cube_54\n",
      "\n",
      "There are 37 points in cube_55 / 100\n",
      "Found 3 clusters in cube_55\n",
      "\n",
      "There are 48 points in cube_56 / 100\n",
      "Found 3 clusters in cube_56\n",
      "\n",
      "There are 37 points in cube_57 / 100\n",
      "Found 3 clusters in cube_57\n",
      "\n",
      "There are 0 points in cube_58 / 100\n",
      "Cube_58 is empty.\n",
      "\n",
      "There are 0 points in cube_59 / 100\n",
      "Cube_59 is empty.\n",
      "\n",
      "There are 44 points in cube_60 / 100\n",
      "Found 3 clusters in cube_60\n",
      "\n",
      "There are 331 points in cube_61 / 100\n",
      "Found 3 clusters in cube_61\n",
      "\n",
      "There are 505 points in cube_62 / 100\n",
      "Found 3 clusters in cube_62\n",
      "\n",
      "There are 421 points in cube_63 / 100\n",
      "Found 3 clusters in cube_63\n",
      "\n",
      "There are 157 points in cube_64 / 100\n",
      "Found 3 clusters in cube_64\n",
      "\n",
      "There are 66 points in cube_65 / 100\n",
      "Found 3 clusters in cube_65\n",
      "\n",
      "There are 57 points in cube_66 / 100\n",
      "Found 3 clusters in cube_66\n",
      "\n",
      "There are 39 points in cube_67 / 100\n",
      "Found 3 clusters in cube_67\n",
      "\n",
      "There are 0 points in cube_68 / 100\n",
      "Cube_68 is empty.\n",
      "\n",
      "There are 0 points in cube_69 / 100\n",
      "Cube_69 is empty.\n",
      "\n",
      "There are 8 points in cube_70 / 100\n",
      "Found 3 clusters in cube_70\n",
      "\n",
      "There are 444 points in cube_71 / 100\n",
      "Found 3 clusters in cube_71\n",
      "\n",
      "There are 2240 points in cube_72 / 100\n",
      "Found 3 clusters in cube_72\n",
      "\n",
      "There are 4562 points in cube_73 / 100\n",
      "Found 3 clusters in cube_73\n",
      "\n",
      "There are 1436 points in cube_74 / 100\n",
      "Found 3 clusters in cube_74\n",
      "\n",
      "There are 75 points in cube_75 / 100\n",
      "Found 3 clusters in cube_75\n",
      "\n",
      "There are 36 points in cube_76 / 100\n",
      "Found 3 clusters in cube_76\n",
      "\n",
      "There are 21 points in cube_77 / 100\n",
      "Found 3 clusters in cube_77\n",
      "\n",
      "There are 0 points in cube_78 / 100\n",
      "Cube_78 is empty.\n",
      "\n",
      "There are 0 points in cube_79 / 100\n",
      "Cube_79 is empty.\n",
      "\n",
      "There are 10 points in cube_80 / 100\n",
      "Found 3 clusters in cube_80\n",
      "\n",
      "There are 91 points in cube_81 / 100\n",
      "Found 3 clusters in cube_81\n",
      "\n",
      "There are 977 points in cube_82 / 100\n",
      "Found 3 clusters in cube_82\n",
      "\n",
      "There are 3293 points in cube_83 / 100\n",
      "Found 3 clusters in cube_83\n",
      "\n",
      "There are 1164 points in cube_84 / 100\n",
      "Found 3 clusters in cube_84\n",
      "\n",
      "There are 16 points in cube_85 / 100\n",
      "Found 3 clusters in cube_85\n",
      "\n",
      "There are 1 points in cube_86 / 100\n",
      "Cube_86 is empty.\n",
      "\n",
      "There are 0 points in cube_87 / 100\n",
      "Cube_87 is empty.\n",
      "\n",
      "There are 0 points in cube_88 / 100\n",
      "Cube_88 is empty.\n",
      "\n",
      "There are 0 points in cube_89 / 100\n",
      "Cube_89 is empty.\n",
      "\n",
      "There are 0 points in cube_90 / 100\n",
      "Cube_90 is empty.\n",
      "\n",
      "There are 2 points in cube_91 / 100\n",
      "Cube_91 is empty.\n",
      "\n",
      "There are 34 points in cube_92 / 100\n",
      "Found 3 clusters in cube_92\n",
      "\n",
      "There are 126 points in cube_93 / 100\n",
      "Found 3 clusters in cube_93\n",
      "\n",
      "There are 52 points in cube_94 / 100\n",
      "Found 3 clusters in cube_94\n",
      "\n",
      "There are 0 points in cube_95 / 100\n",
      "Cube_95 is empty.\n",
      "\n",
      "There are 0 points in cube_96 / 100\n",
      "Cube_96 is empty.\n",
      "\n",
      "There are 0 points in cube_97 / 100\n",
      "Cube_97 is empty.\n",
      "\n",
      "There are 0 points in cube_98 / 100\n",
      "Cube_98 is empty.\n",
      "\n",
      "There are 0 points in cube_99 / 100\n",
      "Cube_99 is empty.\n",
      "\n",
      "\n",
      "Created 495 edges and 222 nodes in 0:00:01.829708.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "graph = mapper.map(projected_X,\n",
    "                   inverse_X=None,\n",
    "                   clusterer=cluster.AgglomerativeClustering(n_clusters=3,\n",
    "                                                             linkage=\"complete\",\n",
    "                                                             affinity=\"cosine\"),\n",
    "                   overlap_perc=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretable inverse X\n",
    "Here we show the flexibility of KeplerMapper by creating an `interpretable_inverse_X` that is easier to interpret by humans.\n",
    "\n",
    "For text, this can be TFIDF (1-3)-wordgrams, like we do here. For structured data this can be regularitory/protected variables of interest, or using another model to select, say, the top 10% features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SHAPE', (11314, 947))\n",
      "('FEATURE NAMES SAMPLE', [u'00', u'000', u'10', u'100', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'1992', u'1993', u'1993apr15', u'20', u'200', u'21', u'22', u'23', u'24', u'25', u'26', u'27', u'28', u'29', u'30', u'31', u'32', u'33', u'34', u'35', u'36', u'37', u'38', u'39', u'40', u'408', u'41', u'42', u'43', u'44', u'45', u'49', u'50', u'500', u'60', u'70', u'80', u'90', u'92', u'93', u'able', u'ac', u'ac uk', u'accept', u'access', u'according', u'acs', u'act', u'action', u'actually', u'add', u'address', u'advance', u'advice', u'ago', u'agree', u'air', u'al', u'allow', u'allowed', u'america', u'american', u'andrew', u'answer', u'anti', u'anybody', u'apparently', u'appears', u'apple', u'application', u'apply', u'appreciate', u'appreciated', u'apr', u'apr 1993', u'apr 93', u'april', u'area', u'aren', u'argument', u'article', u'article 1993apr15', u'ask', u'asked', u'asking', u'assume', u'att', u'att com', u'au', u'available', u'average', u'avoid', u'away', u'bad', u'base', u'baseball', u'based', u'basic', u'basically', u'basis', u'bbs', u'believe', u'best', u'better', u'bible', u'big', u'bike', u'bit', u'bitnet', u'black', u'blue', u'board', u'bob', u'body', u'book', u'books', u'bought', u'box', u'break', u'brian', u'bring', u'brought', u'btw', u'build', u'building', u'built', u'bus', u'business', u'buy', u'ca', u'ca lines', u'california', u'called', u'came', u'canada', u'car', u'card', u'cards', u'care', u'carry', u'cars', u'case', u'cases', u'cause', u'cc', u'center', u'certain', u'certainly', u'chance', u'change', u'changed', u'cheap', u'check', u'chicago', u'children', u'chip', u'choice', u'chris', u'christ', u'christian', u'christians', u'church', u'city', u'claim', u'claims', u'class', u'clear', u'clearly', u'cleveland', u'clinton', u'clipper', u'close', u'cmu', u'cmu edu', u'code', u'college', u'color', u'colorado', u'com', u'com organization', u'com writes', u'come', u'comes', u'coming', u'comment', u'comments', u'common', u'communications', u'comp', u'company', u'complete', u'completely', u'computer', u'computer science', u'computing', u'condition', u'consider', u'considered', u'contact', u'continue', u'control', u'copy', u'corp', u'corporation', u'correct', u'cost', u'couldn', u'country', u'couple', u'course', u'court', u'cover', u'create', u'created', u'crime', u'cs', u'cso', u'cso uiuc', u'cso uiuc edu', u'cup', u'current', u'currently', u'cut', u'cwru', u'cwru edu', u'data', u'date', u'dave', u'david', u'day', u'days', u'dead', u'deal', u'death', u'decided', u'defense', u'deleted', u'department', u'dept', u'design', u'designed', u'details', u'development', u'device', u'did', u'didn', u'die', u'difference', u'different', u'difficult', u'directly', u'disclaimer', u'discussion', u'disk', u'display', u'distribution', u'distribution na', u'distribution na lines', u'distribution usa', u'distribution usa lines', u'distribution world', u'distribution world nntp', u'distribution world organization', u'division', u'dod', u'does', u'does know', u'doesn', u'doing', u'don', u'don know', u'don think', u'don want', u'dos', u'doubt', u'dr', u'drive', u'driver', u'drivers', u'early', u'earth', u'easily', u'east', u'easy', u'ed', u'edu', u'edu article', u'edu au', u'edu david', u'edu organization', u'edu organization university', u'edu reply', u'edu subject', u'edu writes', u'effect', u'email', u'encryption', u'end', u'engineering', u'entire', u'error', u'especially', u'evidence', u'exactly', u'example', u'excellent', u'exist', u'exists', u'expect', u'experience', u'explain', u'expressed', u'extra', u'face', u'fact', u'faith', u'family', u'fan', u'faq', u'far', u'fast', u'faster', u'fax', u'federal', u'feel', u'figure', u'file', u'files', u'final', u'finally', u'fine', u'folks', u'follow', u'following', u'force', u'forget', u'form', u'frank', u'free', u'friend', u'ftp', u'future', u'game', u'games', u'gave', u'general', u'generally', u'germany', u'gets', u'getting', u'given', u'gives', u'giving', u'gmt', u'god', u'goes', u'going', u'gone', u'good', u'got', u'gov', u'government', u'graphics', u'great', u'greatly', u'ground', u'group', u'groups', u'guess', u'gun', u'guns', u'guy', u'half', u'hand', u'happen', u'happened', u'happens', u'happy', u'hard', u'hardware', u'haven', u'having', u'head', u'hear', u'heard', u'heart', u'hell'])\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                      strip_accents=\"unicode\",\n",
    "                      stop_words=\"english\",\n",
    "                      ngram_range=(1,3),\n",
    "                      max_df=0.97,\n",
    "                      min_df=0.02)\n",
    "\n",
    "interpretable_inverse_X = vec.fit_transform(X).toarray()\n",
    "interpretable_inverse_X_names = vec.get_feature_names()\n",
    "\n",
    "print(\"SHAPE\", interpretable_inverse_X.shape)\n",
    "print(\"FEATURE NAMES SAMPLE\", interpretable_inverse_X_names[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "We use `interpretable_inverse_X` as the `inverse_X` during visualization. This way we get cluster statistics that are more informative/interpretable to humans (chargrams vs. wordgrams).\n",
    "\n",
    "We also pass the `projected_X` to get cluster statistics for the projection. For `custom_tooltips` we use a textual description of the label.\n",
    "\n",
    "The color function is simply the multi-class ground truth represented as a non-negative integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendrikvanveen/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote visualization to: newsgroups20.html\n"
     ]
    }
   ],
   "source": [
    "html = mapper.visualize(graph,\n",
    "                        inverse_X=interpretable_inverse_X,\n",
    "                        inverse_X_names=interpretable_inverse_X_names,\n",
    "                        path_html=\"newsgroups20.html\",\n",
    "                        projected_X=projected_X,\n",
    "                        projected_X_names=[\"ISOMAP1\", \"ISOMAP2\"],\n",
    "                        title=\"Newsgroups20: Latent Semantic Char-gram Analysis with Isometric Embedding\",\n",
    "                        custom_tooltips=np.array([target_names[ys] for ys in y]),\n",
    "                        color_values=y)\n",
    "# jupyter.display(\"newsgroups20.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://i.imgur.com/3G4sm4Y.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
